{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How to handle Unbalanced Dataset in a Credit Scoring Model\n# Resampling Methods + Stratified Cross-Validation"},{"metadata":{},"cell_type":"markdown","source":"## Prepare Workspace"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Upload Libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nimport itertools\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Upload Dataset\ndataset = pd.read_csv('../input/hmeq-data/hmeq.csv')\n# Target variable\ny = dataset.BAD\ndataset.drop(['BAD'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summarize Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimensions of dataset\nprint(dataset.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns of dataset\ndataset.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list types for each attribute\ndataset.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a peek at the first rows of the data\ndataset.head(50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize attribute distributions for data frame\nprint(dataset.describe().T)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.info())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rstr(dataset): return dataset.shape, dataset.apply(lambda x: [x.unique()])\nprint(rstr(dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the level of each feature\nfor column in dataset.columns:\n    print(column, dataset[column].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values both to numeric features and categorical features \nfeat_missing = []\n\nfor f in dataset.columns:\n    missings = dataset[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings/dataset.shape[0]\n        \n        # printing summary of missing values\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n\n# how many variables do present missing values?\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing missing values \ndataset = dataset.fillna(method='ffill')\ndataset = dataset.fillna(method='bfill')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Variable Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the class distribution\ny = y.astype(object) \ncount = pd.crosstab(index = y, columns=\"count\")\npercentage = pd.crosstab(index = y, columns=\"frequency\")/pd.crosstab(index = y, columns=\"frequency\").sum()\npd.concat([count, percentage], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=y, data=dataset).set_title(\"Target Variable Distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Variables Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical features\ncategorical_cols = [cname for cname in dataset.columns if\n                    dataset[cname].dtype in ['object']]\ncat = dataset[categorical_cols]\ncat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizations\nsns.set( rc = {'figure.figsize': (5, 5)})\nfcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(x=cat[col], data=cat, palette=\"Set3\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encode the data\nHOX_dataset = pd.get_dummies(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Variables Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical features\nnumerical_cols = [cname for cname in dataset.columns if\n                 dataset[cname].dtype in ['float']]\nnum = dataset[numerical_cols]\nnum.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizations\nsns.set( rc = {'figure.figsize': (5, 5)})\nfnum = ['MORTDUE','VALUE','YOJ','DEROG','CLAGE','DEBTINC','DELINQ','NINQ','CLNO']\n\nfor col in fnum:\n    plt.figure()\n    x=num[col]\n    sns.distplot(x, bins=10)\n    plt.xticks(rotation=45)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part IV - Sampling Techniques + Stratified Cross-Validation"},{"metadata":{},"cell_type":"markdown","source":"## Split Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Break off train and validation set from training data\ny = y.astype('int') \nX_train, X_valid, y_train, y_valid = train_test_split(HOX_dataset, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Over-Sampling + Stratified Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"ros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\nprint(sorted(Counter(y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/ajay1735/my-credit-scoring-model\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Models "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test options and evaluation metric\n\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression(random_state=0)))\nmodels.append(('Bagging', BaggingClassifier(random_state=0)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state=0)))\nmodels.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\n#models.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_resampled, y_resampled)\n    predictions_t = my_model.predict(X_resampled) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_resampled, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    # Computing Confusion matrix for the above algorithm\n    cnf_matrix = confusion_matrix(y_valid, predictions_v)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score.append(f_dict)\nplt.show()    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaled Baseline Models "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot Check Algorithms with standardized dataset\npipelines = []\npipelines.append(('Scaled_LogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression', LogisticRegression(random_state=0))])))\npipelines.append(('Scaled_Bagging', Pipeline([('Scaler', StandardScaler()),('Bagging', BaggingClassifier(random_state=0))])))\npipelines.append(('Scaled_RandomForest', Pipeline([('Scaler', StandardScaler()),('RandomForest', RandomForestClassifier(random_state=0))])))\npipelines.append(('Scaled_AdaBoost', Pipeline([('Scaler', StandardScaler()),('AdaBoost', AdaBoostClassifier(random_state=0))])))\npipelines.append(('Scaled_GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('Scaled_XGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('Scaled_NeuralNetwork', Pipeline([('Scaler', StandardScaler()),('NeuralNetwork', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_resampled, y_resampled)\n    predictions_t = my_model.predict(X_resampled) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_resampled, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    # Computing Confusion matrix for the above algorithm\n    cnf_matrix = confusion_matrix(y_valid, predictions_v)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score_sd.append(f_dict)\nplt.show()   \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_sd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Break off train and validation set from training data\ny = y.astype('int') \nX_train_, X_valid_, y_train_, y_valid_ = train_test_split(HOX_dataset, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Down-Sampling + Stratified Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"rus = RandomUnderSampler(random_state=0)\nX_resampled_, y_resampled_ = rus.fit_resample(X_train_, y_train_)\nprint(sorted(Counter(y_resampled_).items()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Models "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test options and evaluation metric\n\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression(random_state=0)))\nmodels.append(('Bagging', BaggingClassifier(random_state=0)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state=0)))\nmodels.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\n#models.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_resampled_, y_resampled_)\n    predictions_t = my_model.predict(X_resampled_) \n    predictions_v = my_model.predict(X_valid_)\n    accuracy_train = accuracy_score(y_resampled_, predictions_t) \n    accuracy_valid = accuracy_score(y_valid_, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    # Computing Confusion matrix for the above algorithm\n    cnf_matrix = confusion_matrix(y_valid_, predictions_v)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score.append(f_dict)\nplt.show()    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaled Baseline Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot Check Algorithms with standardized dataset\npipelines = []\npipelines.append(('Scaled_LogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression', LogisticRegression(random_state=0))])))\npipelines.append(('Scaled_Bagging', Pipeline([('Scaler', StandardScaler()),('Bagging', BaggingClassifier(random_state=0))])))\npipelines.append(('Scaled_RandomForest', Pipeline([('Scaler', StandardScaler()),('RandomForest', RandomForestClassifier(random_state=0))])))\npipelines.append(('Scaled_AdaBoost', Pipeline([('Scaler', StandardScaler()),('AdaBoost', AdaBoostClassifier(random_state=0))])))\npipelines.append(('Scaled_GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('Scaled_XGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('Scaled_NeuralNetwork', Pipeline([('Scaler', StandardScaler()),('NeuralNetwork', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_resampled_, y_resampled_)\n    predictions_t = my_model.predict(X_resampled_) \n    predictions_v = my_model.predict(X_valid_)\n    accuracy_train = accuracy_score(y_resampled_, predictions_t) \n    accuracy_valid = accuracy_score(y_valid_, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    # Computing Confusion matrix for the above algorithm\n    cnf_matrix = confusion_matrix(y_valid_, predictions_v)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score_sd.append(f_dict)\nplt.show()   \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_sd)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}